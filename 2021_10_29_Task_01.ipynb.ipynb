{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NzyqehHjUlG"
   },
   "source": [
    "# ML in Cybersecurity: Task I\n",
    "\n",
    "## Team\n",
    "  * **Team name**:  *fill this in*\n",
    "  * **Members**:  *fill this in. format: name1 (email1), name2 (email2), ...*\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 11th November 2021, 23:59:59 (email the completed notebook including outputs to mlcysec_ws2022_staff@lists.cispa.saarland)\n",
    "  * Please include your team name and the task number in the file name and the email subject\n",
    "  * Complete this in **teams of 3**\n",
    "  * Feel free to use the forum or the mailing list to find group members.\n",
    "  \n",
    "## Timeline\n",
    "  * 29-Oct-2021: Task 1 hand-out\n",
    "  * **11-Nov-2021** (23:59:59): Email the completed notebook including outputs to mlcysec_ws2022_staff@lists.cispa.saarland\n",
    "  * 12-Nov-2021: Task 1 discussion and summary\n",
    "  \n",
    "  \n",
    "## About this task\n",
    "In this task, you'll implement a digit classifier, based on the popular [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. The dataset is based on a seminal [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), which immensely popularized (convolutional) neural networks. This is a great starting point for ML research and this dataset/model has been a stepping stone numerous other tasks such as [GANs](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), [Adversarial Perturbations](https://arxiv.org/abs/1412.6572) and so many more!\n",
    "\n",
    "This dataset consists of data $\\mathcal{D} = \\{x_i, y_i\\}_{i=1}^N$, where $x_i$ is a 28x28 pixel grayscale image and $y_i$ is a scalar represeting digits between 0-9. The notebook will guide you to load this data, implement classifiers $\\hat{y_i} = f_w(x_i)$  and analyze results. By doing so, you'll have a ML model that works on real data!\n",
    "\n",
    "To put things into context, have a look at Slide 21 in the [second](https://cms.cispa.saarland/mlcysec19/dl/4/2019-10-24-ml.pdf) lecture. Within this framework, the following blocks of this task are fixed:\n",
    "  * *Real-world problem*: Digit classification\n",
    "  * *Performance metric*: Mean accuracy i.e., $ \\frac{1}{N} \\sum_{i=1}^N \\mathbb{1}[\\hat{y_i} = y_i]$, where $\\mathbb{1}[\\hat{y_i} = y_i]$ is 1 if your model predicted the right digit for the $i$-th digit and 0 otherwise.\n",
    "  * *Data*: The MNIST dataset\n",
    "\n",
    "You'll make the the following design-choices:\n",
    " * *Choice of Model*: A model family (Non-parametric methods, Linear classifiers, Neural Networks, etc.)\n",
    " * *ML Model*: Specific model (e.g., SVM with a polynomial kernel)\n",
    " * *Loss/Risk*\n",
    " * *Optimization*\n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The grading for this task will depend on:\n",
    " 1. Functional digit classifier\n",
    "   * Following a well-defined ML pipeline\n",
    "   * Developing 3 classification models (keep them diverse and ideally of increasing complexity)\n",
    "   * Obtaining reasonable accuracies (>80%) on a held-out test set\n",
    " 1. Analysis\n",
    "   * Which methods work better than the rest and why?\n",
    "   * Which hyper-parameters and design-choices were important in each of your methods?\n",
    "   * Quantifying influence of these hyper-parameters on loss and/or validation accuracies\n",
    "   * Trade-offs between methods, hyper-parameters, design-choices\n",
    "    * Anything else you find interesting (this part is open-ended)\n",
    "  \n",
    " A note on (1.): \n",
    "  * Choose your models that aids good insights. We require at least one non-Neural Network (e.g., SVM, KNN) and one Neural Network model (e.g., MLP, CNN).\n",
    "  * We definitely don't expect all three models to achieve >99% test accuracies!\n",
    "\n",
    "## Grading Details\n",
    " * 5 points for loading and visualization \n",
    " * 25x3 points for models. Per model:\n",
    "   * 4 points for written description \n",
    "   * 7 points for implementation\n",
    "   * 7 points for evaluation\n",
    "   * 7 points for summary\n",
    " * 15 points for final summary (Section 3)\n",
    " * 5 points for clean code\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "\n",
    "**The notebook is your task report. So, to make the report readable, omit code for techniques/models/things that did not work. You can use final summary to provide report about these codes.**\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    " ## Versions\n",
    "  * v2.0: Added pytorch\n",
    "  * v1.1: Added Code of Honor\n",
    "  * v1.0: Initial notebook\n",
    "  \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ewNwfFvbFaR"
   },
   "outputs": [],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "640GrzbOevr0"
   },
   "outputs": [],
   "source": [
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "\n",
    "# We only support sklearn and pytorch.\n",
    "\n",
    "# Please set random seed to have reproduceable results, e.g. torch.manual_seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxi-lLD0mKHD"
   },
   "source": [
    "Helpers\n",
    "\n",
    "In case you choose to have some methods you plan to reuse during the notebook, define them here. This will avoid clutter and keep rest of the notebook succinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBbigqdEmKd8"
   },
   "outputs": [],
   "source": [
    "def identity_func(foo):\n",
    "    return foo\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "# You can use this function to flatten 2D inputs\n",
    "def flatten_input_pixels(x_input):\n",
    "    result = []\n",
    "    for i in range(len(x_input)):\n",
    "        result.append(x_input[i].flatten())\n",
    "    return np.array(result, np.uint8)  # [n_samples, n_features]\n",
    "\n",
    "\n",
    "# You can use this function to plot the accuracy of the models with different parametes\n",
    "def plot_scores(x, y, title = \"Title\", x_label = \"X\", y_label = \"Y\"):\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=1)\n",
    "\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_ylim(0.6, 1.0)\n",
    "\n",
    "# You can use this function to visualize input images and the predictions of your models\n",
    "# \"y_pred\" is output of your model \n",
    "# \"n_val\" is number of instances in test or validation sets\n",
    "def vis_predictions(x_eval, y_pred, n_val):\n",
    "    rows, cols = 4, 3\n",
    "\n",
    "    fig,ax = plt.subplots(nrows = rows, ncols = cols)\n",
    "\n",
    "    ids = np.random.randint(0,n_val,rows*cols)\n",
    "    for i in range(cols):   \n",
    "        for j in range(rows):\n",
    "            ax[j][i].set_title('predicted label: {0}'. format(y_pred[ids[(i*rows)+j]]))\n",
    "            two_d = (np.reshape(x_eval[ids[(i*rows)+j]], (28, 28))).astype(np.uint8)\n",
    "            ax[j][i].imshow(two_d)\n",
    "            ax[j][i].axes.get_xaxis().set_visible(False)\n",
    "            ax[j][i].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "    plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1pcmKkyjT7y"
   },
   "source": [
    "# 1. Loading and Visualizing data (5 points)\n",
    "\n",
    "In this section, you'll need to prepare the MNIST data for the experiments you'll be conducting for the remainder of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIU9Q762fmoT"
   },
   "source": [
    "## 1.1. Load Data\n",
    "\n",
    "Here you'll load the MNIST data into memory. The end-goal is to two have the following variables:\n",
    "  * `x_trainval`, `x_test`: of shape $N \\times d_1 \\times d_2 \\dots$ (e.g., $N \\times 784$. 784 since you could flatten each 28x28 pixel image into a single vector)\n",
    "  * `y_trainval`, `y_test`: of shape $N \\times K$ (K = 1 or 10 depending on how you plan to represent the ground-truth digit annotation)\n",
    "\n",
    "You can either do this by:\n",
    "  1. Downloading the MNIST dataset, unpacking and preparing it yourself to have fine-grained control\n",
    "  1. Using high-level existing functions, such as the one provided by  [`torchvision.datasets`](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist).\n",
    "  \n",
    "  \n",
    "  In either case, it is important that you have disjoint train, val, and test splits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kYacpo_jvao"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "print('x_trainval.shape = {},  y_trainval.shape = {}'.format(x_trainval.shape, y_trainval.shape))\n",
    "print('x_test.shape = {},  y_test.shape = {}'.format(x_test.shape, y_test.shape))\n",
    "\n",
    "#\n",
    "# Feel free to have multiple variables in case your models are designed for different formats\n",
    "# For instance, in case your model requires Nx28x28 inputs, declare x_trainval_3d, etc.\n",
    "\n",
    "# Tip: Set this to a tiny number (such 0.05) to aid debugging\n",
    "# After all, you do not want to train/evaluate on the entire dataset to find bugs\n",
    "DEBUG_FRAC = 1.0\n",
    "x_trainval = x_trainval[:int(len(x_trainval)*DEBUG_FRAC)]\n",
    "y_trainval = y_trainval[:int(len(y_trainval)*DEBUG_FRAC)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA6_cejNjzYw"
   },
   "source": [
    "#### 1.2. Visualize Data\n",
    "\n",
    "To get the hang of your data you'll be training a digit classifier on, visualize it.\n",
    "\n",
    "Examples of ways to visualize it:\n",
    "  * Given a digit, display few randomly sampled images for this digit (the bare minimum)\n",
    "  * Visualize as a grid (e.g., Slide 4, [Lecture 2](https://cms.cispa.saarland/mlcysec19/dl/4/2019-10-24-ml.pdf)) using a combination of `plt.imshow` and `plt.subplots`\n",
    "  \n",
    "It's up to you to decide how you want to do this. The end-goal is for you to potentially give a trailer of the dataset to someone who hasn't seen it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dISIbt4plyoD"
   },
   "outputs": [],
   "source": [
    "# Visualize 10 examples of 10 classes. You can extend the following code:\n",
    "rows, cols = 10, 10\n",
    "fig,ax = plt.subplots(nrows = rows, ncols = cols)\n",
    "\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "plt.savefig('fig1.pdf')   # Save the figures\n",
    "plt.show()   # These should be some visualization of data at the end of this section\n",
    "\n",
    "# You can see an output example in the follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8sAT53jmJ8_"
   },
   "source": [
    "# 2. Digit classifiers\n",
    "\n",
    "In this section, you'll begin developing models to perform digit classification.\n",
    "\n",
    "Each model needs to be structured like so:\n",
    "  1. Give a brief reason which model you are going to train and why you choose it\n",
    "  1. Define hyper-parameters for model and optimization procedure\n",
    "  1. Define your model\n",
    "  1. Define optimization method and fit model to data\n",
    "  1. Summarize your findings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkF-7eFnpWoe"
   },
   "source": [
    "## 2.1: Model [M1]: *fill-this-in* (25 points)\n",
    "\n",
    "**Short description **: *fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVyT9Oddp3GB"
   },
   "source": [
    "### 2.1.1: Hyper-parameters\n",
    "\n",
    "Define hyper-parameters for your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuHt4T7Vp5NC"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "degree = np.asarray([1,2,3]) # example\n",
    "\n",
    "\n",
    "test_set = 'val'  #  or 'test'\n",
    "# Decide all your hyperparameters based on validation performance\n",
    "# Then, switch to 'test' for final evaluation\n",
    "\n",
    "if test_set == 'val':\n",
    "    train_idxs, val_idxs = ..., ...   # Fill in\n",
    "    x_train, y_train = x_trainval[train_idxs], y_trainval[train_idxs]\n",
    "    x_eval, y_eval = x_trainval[val_idxs], y_trainval[val_idxs]\n",
    "else:\n",
    "    x_train, y_train = x_trainval, y_trainval\n",
    "    x_eval, y_eval = x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZhVgrFM01FF"
   },
   "source": [
    "### 2.1.2: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzJ1X7ek01FF"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# e.g. normalize, flatten input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkuCgPatp59X"
   },
   "source": [
    "### 2.1.3: Model\n",
    "\n",
    "Define your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qV3SuPAp6XF"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxE6d6OXp6sU"
   },
   "source": [
    "### 2.1.4: Fit Model\n",
    "\n",
    "Define optimization procedure and fit your model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08tLwuchp68-"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# \n",
    "#  Please save the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaJv_d_Dp7OM"
   },
   "source": [
    "### 2.1.5: Evaluation\n",
    "\n",
    "Evaluate your model.\n",
    "  * Evaluate models with different parameters \n",
    "  * Plot the score (accuracy) for each model using \"plot_scores\" function\n",
    "  * Report score for the best model\n",
    "  * Use \"vis_predictions\" function to visualize few examples of test/validation set with the corresponding predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZtLgPZrp7h5"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# Example: y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFnNiFCI01FH"
   },
   "outputs": [],
   "source": [
    "# Here plot score (accuracy) for each model. You can use \"plot_scores\" function.\n",
    "\n",
    "# Example: plot_scores(parameters, scores, \"title\", \"x_label\", \"y_label\"), \n",
    "\n",
    "# You can see an example in the follow.\n",
    "# Note that the visualizations/plots provided are just simple examples/illustrations. \n",
    "# We encourage more informative and alternate methods to present results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZ_OYghL01FH"
   },
   "outputs": [],
   "source": [
    "# Here report the score for the best model\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXygRW8D01FH"
   },
   "outputs": [],
   "source": [
    "# Visualize the predictions\n",
    "# Example: vis_predictions(x_eval, y_pred, size_of_data)\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEQrdyLHsUIu"
   },
   "source": [
    "### 2.1.6: Summary\n",
    "\n",
    "Summarize your findings:\n",
    " * Which hyper-parameters were important and how did they influence your results?\n",
    " * What were other design choices you faced?\n",
    " * Any other interesting insights..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fcq52WUUs2Mm"
   },
   "source": [
    "# 2.2: Model [M2]: *fill-this-in* (25 points)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XPGNsr701FI"
   },
   "source": [
    "### 2.2.1: Hyper-parameters\n",
    "\n",
    "Define hyper-parameters for your method here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V-aANfA01FI"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "degree = np.asarray([1,2,3]) # example\n",
    "\n",
    "# You don't need the following code, if you can use data from 2.1.1.\n",
    "\n",
    "test_set = 'val'  #  or 'test'\n",
    "# Decide all your hyperparameters based on validation performance\n",
    "# Then, switch to 'test' for final evaluation\n",
    "\n",
    "if test_set == 'val':\n",
    "    train_idxs, val_idxs = ..., ...   # Fill in\n",
    "    x_train, y_train = x_trainval[train_idxs], y_trainval[train_idxs]\n",
    "    x_eval, y_eval = x_trainval[val_idxs], y_trainval[val_idxs]\n",
    "else:\n",
    "    x_train, y_train = x_trainval, y_trainval\n",
    "    x_eval, y_eval = x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nure4xC_01FK"
   },
   "source": [
    "### 2.2.2: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ak3VMb8S01FK"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# e.g. normalize, flatten input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oHcWedF01FK"
   },
   "source": [
    "### 2.2.3: Model\n",
    "\n",
    "Define your model here (all hyper-parameters in 2.1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RalMhJvZ01FK"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5k8P8PP01FL"
   },
   "source": [
    "### 2.2.4: Fit Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DS0sDmha01FL"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# \n",
    "#  Please save the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNQKy7a901FL"
   },
   "source": [
    "### 2.2.5: Evaluation\n",
    "\n",
    "Evaluate your model.\n",
    "  * Evaluate models with different parameters \n",
    "  * Plot score (accuracy) for each model using \"plot_scores\" function\n",
    "  * Report the score for the best model\n",
    "  * Use \"vis_predictions\" function to visualize few examples of test/validation set with the corresponding predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cac5u-Lk01FL"
   },
   "outputs": [],
   "source": [
    "# Here plot score (accuracy) for each model. You can use \"plot_scores\" function.\n",
    "\n",
    "# Example: plot_scores(parameters, scores, \"title\", \"x_label\", \"y_label\"), \n",
    "\n",
    "# You can see an example in the follow.\n",
    "# Note that the visualizations/plots provided are just simple examples/illustrations. \n",
    "# We encourage more informative and alternate methods to present results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D68bsuVB01FL"
   },
   "outputs": [],
   "source": [
    "# Here report the score for the best model\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zgw4JUH01FL"
   },
   "outputs": [],
   "source": [
    "# Visualize the predictions\n",
    "# Example: vis_predictions(x_eval, y_pred, size_of_data)\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOT67VC801FL"
   },
   "source": [
    "### 2.2.6: Summary\n",
    "\n",
    "Summarize your findings:\n",
    " * Which hyper-parameters were important and how did they influence your results?\n",
    " * What were other design choices you faced?\n",
    " * Any other interesting insights..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgZ547yT01FM"
   },
   "source": [
    "# 2.3: Model [M3] (Neural Networks): *fill-this-in* (25 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFslEvfu01FM"
   },
   "source": [
    "### 2.3.1: Hyper-parameters\n",
    "\n",
    "Define hyper-parameters for your method here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzSHcgtr01FM"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "batch_size_train = ... # Fill in\n",
    "batch_size_test = ... # Fill in\n",
    "n_epochs = ... # Fill in\n",
    "# other parameters ...\n",
    "\n",
    "\n",
    "# You don't need the following code, if you can use data from 2.1.1.\n",
    "# You can also use torch built-in functions (torch.utils.data)\n",
    "\n",
    "test_set = 'val'  #  or 'test'\n",
    "# Decide all your hyperparameters based on validation performance\n",
    "# Then, switch to 'test' for final evaluation\n",
    "\n",
    "if test_set == 'val':\n",
    "    train_idxs, val_idxs = ..., ...   # Fill in\n",
    "    x_train, y_train = x_trainval[train_idxs], y_trainval[train_idxs]\n",
    "    x_eval, y_eval = x_trainval[val_idxs], y_trainval[val_idxs]\n",
    "else:\n",
    "    x_train, y_train = x_trainval, y_trainval\n",
    "    x_eval, y_eval = x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIkrTWQ001FM"
   },
   "source": [
    "### 2.3.2: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p13RLv7u01FM"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# e.g. normalize, flatten input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cp0nyS8C01FM"
   },
   "source": [
    "### 2.3.3: Model\n",
    "\n",
    "Define your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6kHJuH401FM"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    #\n",
    "    #\n",
    "    # ------- Your Code -------\n",
    "    #\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhzlip-t01FM"
   },
   "source": [
    "### 2.2.4: Fit Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUWeF4mQ01FM"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# Example: net = Net(), ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARJe7dZM01FM"
   },
   "outputs": [],
   "source": [
    "def train(args ... # Fill in):\n",
    "    #\n",
    "    #\n",
    "    # ------- Your Code -------\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qn0MgIyX01FN"
   },
   "outputs": [],
   "source": [
    "def test(args ... # Fill in):\n",
    "    #\n",
    "    #\n",
    "    # ------- Your Code -------\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rgwj4e0901FN"
   },
   "outputs": [],
   "source": [
    "# Save your model using torch.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aISdfCpb01FN"
   },
   "source": [
    "### 2.2.5: Evaluation\n",
    "\n",
    "Evaluate your model.\n",
    "\n",
    "  * Loss curves: Plot epoch (# passes over training data) and loss\n",
    "  * Accuracy curves: Plot epoch and accuracy over val/test set\n",
    "  * Final numbers: Report final accuracy numbers for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urtpEDvx01FN"
   },
   "outputs": [],
   "source": [
    "# Here plot epoch (# passes over training data) and loss\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAiMSqqs01FN"
   },
   "outputs": [],
   "source": [
    "# Here plot epoch and accuracy over val/test set\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTZJPZIP01FN"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "# Example:\n",
    "# net = Net()\n",
    "# net.load_state_dict(torch.load(\"PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzQvt7IF01FN"
   },
   "outputs": [],
   "source": [
    "# Here report the score for the best model\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAcVb-iK01FN"
   },
   "outputs": [],
   "source": [
    "# Visualize the predictions\n",
    "# Example: vis_predictions(x_eval, y_pred, size_of_data)\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPMall2f01FN"
   },
   "source": [
    "### 2.3.6: Summary\n",
    "\n",
    "Summarize your findings:\n",
    " * Which hyper-parameters were important and how did they influence your results?\n",
    " * What were other design choices you faced?\n",
    " * Any other interesting insights..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex3qQp3JolD1"
   },
   "source": [
    "# 3. Summary (20 points)\n",
    "\n",
    "Enter your final summary here.\n",
    "\n",
    "You should now compare performance  on the three models [M1], [M2] and [M3]. Present this in a tabular format and/or using plots.\n",
    "\n",
    "Which model do you recommend to perform digit classification and why?\n",
    "\n",
    "Feel free to discuss other insightful observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pa6rPT53LUW8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Task_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
